








!which python





from langchain_community.chat_models import ChatOllama

llm = ChatOllama(model="llama3", temperature=0)
response = llm.invoke("who wrote A Song of Ice and Fire?")
print(response.content)









from langchain_community.chat_models import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate 

llm = ChatOllama(
    model="llama3", 
    # keep_alive=-1,
    
    # You can experiment with the following parameters:
    temperature=0,
    max_new_tokens=512
)
output_parser = StrOutputParser()





prompt = PromptTemplate.from_template("Write me the lyrics for a 30 seconds jingle about {product}")

# using LangChain Expressive Language chain syntax (LCEL)
chain = prompt | llm | output_parser





print(chain.invoke({"product": "home insurance"}))





for chunk in chain.stream({"product": "cat food"}):
    print(chunk, end="", flush=True)





import json
from langchain_core.output_parsers import JsonOutputParser

output_schema = {
    "title": "Person",
    "description": "Identifying information about a person.",
    "type": "object",
    "properties": {
        "name": {"title": "Name", "description": "The person's name", "type": "string"},
        "age": {"title": "Age", "description": "The person's age", "type": "integer"},
        "favorite_food": {
            "title": "Fav Food",
            "description": "The person's favorite food",
            "type": "string",
        },
    },
    "required": ["name", "age"],
}

output_schema_as_string = json.dumps(output_schema, indent=2)


llm = ChatOllama(
    model="llama3",
    format="json",
    temperature=0.1,
    max_new_tokens=512
    )





from langchain_core.prompts import ChatPromptTemplate 

messages = [
    ("system", 
         "You are a helpful assistant that will extract information about a person and produce "
         "an output using the following json schema: {json_schema}"),
    ("human", "{person_info}"),
]

prompt = ChatPromptTemplate.from_messages(messages)
chain = prompt | llm | JsonOutputParser()


response = chain.invoke(
    {
        "json_schema": output_schema_as_string,
        "person_info": "Cesar is 37 and loves sushi"
    })

print(response)
print(type(response))





from langchain_core.prompts import ChatPromptTemplate 
from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate

messages = [
    SystemMessagePromptTemplate.from_template(
         "You are a helpful assistant that will extract information about a person and produce "
         "an output using the following json schema: {json_schema}"),
    HumanMessagePromptTemplate.from_template("{person_info}"),
]

prompt = ChatPromptTemplate.from_messages(messages)
chain = prompt | llm | JsonOutputParser()





chain_str_parser = prompt | llm | StrOutputParser()
response = chain_str_parser.invoke(
    {
        "json_schema": output_schema_as_string,
        "person_info": "Cesar is 37 and loves sushi"
    })
print(response)
print(type(response))


response





person_info_text = """As Laura walked into the cozy caf√©, her bright smile illuminated the room, and 
her sparkling eyes seemed to dance with a youthful energy. Her long, curly brown hair bounced with 
each step, framing her heart-shaped face and emphasizing her petite nose ring. She was dressed in 
a flowy sundress, its vibrant floral pattern reflecting her playful personality. As she scanned the 
menu, her eyes widened with excitement, and she couldn't help but let out a little squeal of delight
when she spotted her favorite dish: a decadent chocolate lava cake. She had always been a 
self-proclaimed chocoholic, and the mere mention of the rich, gooey treat was enough to transport 
her back to her childhood days of baking with her grandmother. With a spring in her step, she 
ordered her beloved dessert and settled in for a delightful afternoon of indulgence, to celebrate  
her 30th birthday."""

response = chain.invoke(
    {
        "json_schema": output_schema_as_string,
        "person_info": person_info_text
    })

response








llm = ChatOllama(
    model="llama3",
    temperature=0.7,
    max_new_tokens=512
)
# First we get the LLM to give us a dish
template = """Your job is to come up with a classic dish from the area that the users suggests.
% USER LOCATION: {user_location}

YOUR RESPONSE:
"""
prompt_location = ChatPromptTemplate.from_template(template)

# Then we use that dish to get the recipe
template = """Given a meal, give a short and simple recipe on how to make that dish at home.
% MEAL: {user_meal}

YOUR RESPONSE:
"""
prompt_meal = ChatPromptTemplate.from_template(template)

chain = prompt_location | llm | prompt_meal | llm | output_parser

response = chain.invoke("Rome")
print(response)





from langchain.chains import LLMChain, SimpleSequentialChain
from langchain.prompts import PromptTemplate

llm = ChatOllama(
    model="llama3",
    temperature=0.7,
    max_new_tokens=512
)

template = """Your job is to come up with a classic dish from the area that the users suggests.
% USER LOCATION: {user_location}

YOUR RESPONSE:
"""
prompt_template = PromptTemplate(input_variables=["user_location"], template=template)
prompt = ChatPromptTemplate.from_template(template)

# Holds the'location' chain
location_chain = LLMChain(llm=llm, prompt=prompt_template)



template = """Given a meal, give a short and simple recipe on how to make that dish at home.
% MEAL: {user_meal}

YOUR RESPONSE:
"""
prompt_template = PromptTemplate(input_variables=["user_meal"], template=template)

# Holds the'meal' chain
meal_chain = LLMChain(llm=llm, prompt=prompt_template)

overall_chain = SimpleSequentialChain(chains=[location_chain, meal_chain], verbose=True)

review = overall_chain.invoke("Rome")

















from langchain_core.prompts import PromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_experimental.llms.ollama_functions import OllamaFunctions

# Pydantic Schema for structured response
class Person(BaseModel):
    name: str = Field(description="The person's name", required=True)
    age: float = Field(description="The person's age", required=True)
    favorite_food: str = Field(description="The person's favorite food")



# Prompt template llama3
prompt = PromptTemplate.from_template("""<|begin_of_text|><|start_header_id|>system<|end_header_id|>
You are a smart assistant take the following context and question below and return your answer in JSON.<|eot_id|>
<|start_header_id|>user<|end_header_id|>
QUESTION: {question} \n
CONTEXT: {context} \n
JSON:
<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
"""
)

# Chain
llm = OllamaFunctions(model="llama3", 
                      format="json", 
                      temperature=0)

structured_llm = llm.with_structured_output(Person)
chain = prompt | structured_llm


context = """Three friends are celebrating their birthday together, eating their favorite food at Pizza Hut. 
Mia is 2 years older than Jake, and Emma is 2 years younger than Jake. Jake is 10 years old.  """

response = chain.invoke({
    "question": "How old is Emma?",
    "context": context
    })

response



